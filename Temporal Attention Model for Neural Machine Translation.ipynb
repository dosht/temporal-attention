{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [WORK IN PROGRESS]\n",
    "# Temporal Attention Model for Neural Machine Translation\n",
    "Unofficial implementation of paper: http://arxiv.org/abs/1608.02927\n",
    "\n",
    "### Requirements:\n",
    " - [Keras](https://github.com/fchollet/keras)\n",
    " - [Tensorflow](https://github.com/tensorflow/tensorflow)\n",
    " - [Theano](https://github.com/Theano/Theano)\n",
    " - https://github.com/farizrahman4u/seq2seq Seq2Seq implemtation built on top of Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n",
      "/opt/conda/lib/python3.5/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading (French, English) language pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.models.rnn.translate import data_utils\n",
    "data_dir = \"/data/translate\" # You may need to change that or create a sympolic link\n",
    "vocab_size = 20000\n",
    "pathes = data_utils.prepare_wmt_data(data_dir, vocab_size, vocab_size)\n",
    "en2_path, fr2_path, en2013_path, fr2013_path, en_vocab_path, fr_vocab_path = pathes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def is_ascii(s):\n",
    "    return all(ord(c) < 128 for c in s)\n",
    "\n",
    "# https://github.com/nicolas-ivanov/tf_seq2seq_chatbot/blob/master/tf_seq2seq_chatbot/lib/data_utils.py\n",
    "\n",
    "_PAD = \"_PAD\"\n",
    "_GO = \"_GO\"\n",
    "_EOS = \"_EOS\"\n",
    "_UNK = \"_UNK\"\n",
    "_START_VOCAB = [_PAD, _GO, _EOS, _UNK]\n",
    "\n",
    "PAD_ID = 0\n",
    "GO_ID = 1\n",
    "EOS_ID = 2\n",
    "UNK_ID = 3\n",
    "\n",
    "# Regular expressions used to tokenize.\n",
    "_WORD_SPLIT = re.compile(\"([.,!?\\\"':;)(])\")\n",
    "_DIGIT_RE = re.compile(r\"\\d{3,}\")\n",
    "\n",
    "def read_vocab(vocab_path):\n",
    "    vocab_list = []\n",
    "    vocab_list.extend(_START_VOCAB)\n",
    "\n",
    "    with open(vocab_path, 'br') as f:\n",
    "        vocab_list.extend([s.decode(\"utf-8\").strip() for s in f.readlines() if is_ascii(s)])\n",
    "\n",
    "    words_to_ids = {w:i for (i, w) in enumerate(vocab_list)}\n",
    "    ids_to_words = {i:w for (w, i) in words_to_ids.items()}\n",
    "    return ids_to_words, words_to_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "en_index, en_vocab = read_vocab(en_vocab_path)\n",
    "fr_index, fr_vocab = read_vocab(fr_vocab_path)\n",
    "\n",
    "def basic_tokenizer(sentence):\n",
    "    \"\"\"Very basic tokenizer: split the sentence into a list of tokens.\"\"\"\n",
    "    words = []\n",
    "    for space_separated_fragment in sentence.strip().split():\n",
    "        words.extend(re.split(_WORD_SPLIT, space_separated_fragment))\n",
    "    return [w.lower() for w in words if w]\n",
    "\n",
    "def sentence_to_token_ids(sentence, vocabulary,\n",
    "                          tokenizer=None, normalize_digits=True):\n",
    "    \"\"\"Convert a string to list of integers representing token-ids.\n",
    "\n",
    "    For example, a sentence \"I have a dog\" may become tokenized into\n",
    "    [\"I\", \"have\", \"a\", \"dog\"] and with vocabulary {\"I\": 1, \"have\": 2,\n",
    "    \"a\": 4, \"dog\": 7\"} this function will return [1, 2, 4, 7].\n",
    "\n",
    "    Args:\n",
    "    sentence: a string, the sentence to convert to token-ids.\n",
    "    vocabulary: a dictionary mapping tokens to integers.\n",
    "    tokenizer: a function to use to tokenize each sentence;\n",
    "      if None, basic_tokenizer will be used.\n",
    "    normalize_digits: Boolean; if true, all digits are replaced by 0s.\n",
    "\n",
    "    Returns:\n",
    "    a list of integers, the token-ids for the sentence.\n",
    "    \"\"\"\n",
    "    if tokenizer:\n",
    "        words = tokenizer(sentence)\n",
    "    else:\n",
    "        words = basic_tokenizer(sentence)\n",
    "    if not normalize_digits:\n",
    "        return [vocabulary.get(w, UNK_ID) for w in words]\n",
    "  \n",
    "    # Normalize digits by 0 before looking words up in the vocabulary.\n",
    "    return [vocabulary.get(re.sub(_DIGIT_RE, \"0\", w), UNK_ID) for w in words]\n",
    "\n",
    "def token_ids_to_sentence(ids, vocab_index):\n",
    "    maybe_words = [vocab_index.get(_id) for _id in ids]\n",
    "    return \" \".join([w for w in maybe_words if w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#FIXME: some non-ascii charachters\n",
    "en_vocab_size = len(en_vocab) + 1\n",
    "fr_vocab_size = len(fr_vocab) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19905\n",
      "19905\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "print(len(en_vocab))\n",
    "print(len(en_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15239, 22, 1511, 614, 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'a is me strategy'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = sentence_to_token_ids(\"A is me strategy stratégie\", en_vocab)\n",
    "print(ids)\n",
    "token_ids_to_sentence(ids, en_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/data/translate/giga-fren.release2.ids20000.en',\n",
       " '/data/translate/giga-fren.release2.ids20000.fr',\n",
       " '/data/translate/newstest2013.ids20000.en',\n",
       " '/data/translate/newstest2013.ids20000.fr',\n",
       " '/data/translate/vocab20000.en',\n",
       " '/data/translate/vocab20000.fr')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pathes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    with open(path, 'r') as f:\n",
    "        return [[int(x) for x in line.split(\" \")] for line in f.read().splitlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# e.g. [59, 3, 610, 9, 6251, 4, 3, 7, 3]\n",
    "en_ids = read_data(en2013_path)\n",
    "fr_ids = read_data(fr2013_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Make it the same length (= the max length of the sentences) with zeros for shorter sentences\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "en_set = pad_sequences(en_ids)\n",
    "fr_set = pad_sequences(fr_ids)\n",
    "en_max_length = en_set.shape[1]\n",
    "fr_max_length = fr_set.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'base mieux génétiquement du _UNK'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids_to_sentence(fr_set[0], fr_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "en_max_features = max(en_vocab.values())\n",
    "fr_max_features = max(fr_vocab.values())\n",
    "embedding_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples: 3000\n",
      "en_max_length: 110\n",
      "fr_max_length: 126\n",
      "en_max_features: 20003\n",
      "fr_max_features: 20003\n",
      "embedding_size: 64\n"
     ]
    }
   ],
   "source": [
    "print(\"number of samples:\", en_set.shape[0])\n",
    "print(\"en_max_length:\", en_max_length)\n",
    "print(\"fr_max_length:\", fr_max_length)\n",
    "print(\"en_max_features:\", en_max_features)\n",
    "print(\"fr_max_features:\", fr_max_features)\n",
    "print(\"embedding_size:\", embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_set.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeding layer for en and fr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Dense, Merge, Dropout, RepeatVector, Permute, Activation, recurrent, LSTM, GRU\n",
    "from keras.models import Sequential\n",
    "from keras.layers.wrappers import TimeDistributed, Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 110, 64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(en_max_features, embedding_size, input_length=en_max_length, mask_zero=True))\n",
    "model.compile('rmsprop', 'mse')\n",
    "en_embed = model.predict(en_set)\n",
    "en_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 126, 64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(fr_max_features, embedding_size, input_length=fr_max_length, mask_zero=True))\n",
    "model.compile('rmsprop', 'mse')\n",
    "fr_embed = model.predict(fr_set)\n",
    "fr_embed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 126, 64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Experiment  1\n",
    "hidden_size = 32\n",
    "model = Sequential()\n",
    "model.add(GRU(hidden_size, input_shape=(en_max_length, embedding_size))) # (3000, 110, 64) -> (3000, 32)\n",
    "model.add(RepeatVector(fr_max_length)) # (3000, 32) -> (3000, 126, 32)\n",
    "# model.add(TimeDistributed(Dense(fr_max_length)))\n",
    "model.add(Bidirectional(GRU(embedding_size, return_sequences=True), merge_mode='sum')) # (3000, 126, 32) -> (3000, 126, 64)\n",
    "model.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 126, 64)\n",
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 42s - loss: 5.4835e-04    \n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 32s - loss: 3.6644e-04    \n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 31s - loss: 3.4644e-04    \n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 32s - loss: 3.3820e-04    \n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 32s - loss: 3.3387e-04    \n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 33s - loss: 3.3094e-04    \n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 32s - loss: 3.2918e-04    \n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 33s - loss: 3.2785e-04    \n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 32s - loss: 3.2678e-04    \n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 32s - loss: 3.2603e-04    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb3fec0ebe0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Experiment  2\n",
    "hidden_size = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(en_max_features, embedding_size, input_length=en_max_length, mask_zero=True))\n",
    "model.add(Bidirectional(GRU(hidden_size), merge_mode='sum'))\n",
    "model.add(RepeatVector(fr_max_length))\n",
    "model.add(GRU(embedding_size, return_sequences=True))\n",
    "print(model.output_shape)\n",
    "model.compile('rmsprop', 'mse')\n",
    "model.fit(en_set, fr_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 28s - loss: 4.2822e-04    \n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 20s - loss: 3.4502e-04    \n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 20s - loss: 3.3201e-04    \n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 19s - loss: 3.2797e-04    \n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 20s - loss: 3.2646e-04    \n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 20s - loss: 3.2522e-04    \n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 20s - loss: 3.2371e-04    \n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 21s - loss: 3.1993e-04    \n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 20s - loss: 2.9921e-04    \n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 20s - loss: 2.8977e-04    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb3c2a97390>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_size = 32\n",
    "model = Sequential()\n",
    "model.add(GRU(hidden_size, input_shape=(en_max_length, embedding_size))) # (3000, 110, 64) -> (3000, 32)\n",
    "model.add(RepeatVector(fr_max_length)) # (3000, 32) -> (3000, 126, 32)\n",
    "model.add(GRU(hidden_size, return_sequences=True)) # (3000, 126, 32) -> (3000, 126, 32)\n",
    "model.add(TimeDistributed(Dense(embedding_size))) # (3000, 126, 32) -> (3000, 126, 64)\n",
    "model.compile('rmsprop', 'mse')\n",
    "\n",
    "model.fit(en_embed, fr_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = model.predict(en_embed[0:1])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model infered from Seq2Seq\n",
    "```python\n",
    "model = Sequential()\n",
    "model.add(LSTM(hidden_dim, return_sequences=True, mask_zero=True))\n",
    "model.add(Dropout(droupout))\n",
    "model.add(LSTM(hidden_dim, )) # Encoder\n",
    "model.add(Dropout(droupout))\n",
    "model.add(RepeatVecotr(output_lenght))\n",
    "model.add(LSTM(hidden_dim, return_sequences=True, )) # Decoder\n",
    "model.add(LSTM(hidden_dim, return_sequences=True, ))\n",
    "model.add(Droupout(droupout))\n",
    "model.add(TimeDistributed(Dense(output_dim)))\n",
    "model.compile('rmsprop', 'mse')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### https://github.com/fchollet/keras/issues/395\n",
    "\n",
    "In the model I listed below with the english sentence as input and the entire french sentence as output. The RNN model will maintain state across each timestep as it predicts the output sentence, no extra work required on your behalf. You will however need to one hot encode and zero pad the output sequence (the french sentence) and have it do a softmax over all possible words for the output at each time step. The ys then are 3D, each row is a matrix of height - number of french words, and width - number of time steps.\n",
    "\n",
    "```python\n",
    "embedding_size = 50\n",
    "hidden_size = 512\n",
    "output_size = 20\n",
    "maxlen = 60\n",
    "\n",
    "model = Sequential()\n",
    "model.add(JZS1(embedding_size, hidden_size)) # try using a GRU instead, for fun\n",
    "model.add(Dense(hidden_size, hidden_size))\n",
    "model.add(Activation('relu'))\n",
    "model.add(RepeatVector(maxlen))\n",
    "model.add(JZS1(hidden_size, hidden_size, return_sequences=True))\n",
    "model.add(TimeDistributedDense(hidden_size, output_size, activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.wrappers import TimeDistributed\n",
    "\n",
    "embedding_size = 64\n",
    "hidden_size = 512\n",
    "embedding_size = EN_REPRESENTATION_SIZE\n",
    "MAX_LEN = fr_set.shape[1]\n",
    "max_features = FR_BOUND\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(EN_REPRESENTATION_SIZE, EMBED_HIDDEN_SIZE, input_length=en_set.shape[1], mask_zero=True))\n",
    "model.add(GRU(hidden_size)) # try using a GRU instead, for fun\n",
    "model.add(Dense(hidden_size))\n",
    "model.add(Activation('relu'))\n",
    "model.add(RepeatVector(MAX_LEN))\n",
    "model.add(GRU(hidden_size, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(max_features, activation=\"softmax\")))\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy seq2seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seq2seq\n",
    "from seq2seq.models import SimpleSeq2seq\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(EN_REPRESENTATION_SIZE, EMBED_HIDDEN_SIZE, input_length=en_set.shape[1]))\n",
    "model = SimpleSeq2seq(\n",
    "        input_dim=EN_REPRESENTATION_SIZE,\n",
    "        input_length=en_max_length,\n",
    "        hidden_dim=50,\n",
    "        output_length=FR_REPRESENTATION_SIZE,\n",
    "        output_dim=fr_max_length)\n",
    "\n",
    "model.compile(loss='mse', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model.fit(en_embed, fr_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# RNN = GRU\n",
    "# EMBED_HIDDEN_SIZE = 50\n",
    "\n",
    "# encoder = Sequential()\n",
    "# encoder.add(Embedding(en_vocab_size, EMBED_HIDDEN_SIZE, input_length=en_max_length))\n",
    "\n",
    "# decoder = Sequential()\n",
    "# decoder.add(Embedding(fr_vocab_size, EMBED_HIDDEN_SIZE, input_length=fr_max_length))\n",
    "\n",
    "# decoder.add(RNN(EMBED_HIDDEN_SIZE, return_sequences=False))\n",
    "# decoder.add(RepeatVector(en_max_length))\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Merge([encoder, decoder], mode='sum'))\n",
    "# model.add(RNN(EMBED_HIDDEN_SIZE, return_sequences=False))\n",
    "# model.add(Dropout(0.3))\n",
    "# model.add(Dense(fr_vocab_size, activation='softmax'))\n",
    "\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# p = model.predict([en_set, fr_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# outputs = [int(np.argmax(logit, axis=0)) for logit in p[0]][0:fr_input_length]\n",
    "# token_ids_to_sentence(outputs, fr_index)\n",
    "# # [fr_ids[output] for output in outputs]\n",
    "# # print(\" \".join([tf.compat.as_str(fr_ids[output]) for output in outputs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
