{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [WORK IN PROGRESS]\n",
    "# Temporal Attention Model for Neural Machine Translation\n",
    "Unofficial implementation of paper: http://arxiv.org/abs/1608.02927\n",
    "\n",
    "### Requirements:\n",
    " - [Keras](https://github.com/fchollet/keras)\n",
    " - [Tensorflow](https://github.com/tensorflow/tensorflow)\n",
    " - [Theano](https://github.com/Theano/Theano)\n",
    " - https://github.com/farizrahman4u/seq2seq Seq2Seq implemtation built on top of Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Dense, Merge, Dropout, RepeatVector, Permute, Activation, recurrent, LSTM, GRU\n",
    "from keras.models import Sequential\n",
    "from keras.layers.wrappers import TimeDistributed, Bidirectional\n",
    "from data_utils import *\n",
    "from dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en vocab size 19905\n",
      "fr vocab size 19915\n",
      "en max lentgh 110\n",
      "fr max length 126\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"/data/translate\"\n",
    "vocab_size = 20000\n",
    "en, fr = prepare_date(data_dir, vocab_size)\n",
    "print(\"en vocab size\", en.vocab_size)\n",
    "print(\"fr vocab size\", fr.vocab_size)\n",
    "print(\"en max lentgh\", en.max_length)\n",
    "print(\"fr max length\", fr.max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeding layer for en and fr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_size = 64\n",
    "en_set = en.matrix\n",
    "fr_set = fr.matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 110, 64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(en.max_features, embedding_size, input_length=en.max_length, mask_zero=True))\n",
    "model.compile('rmsprop', 'mse')\n",
    "en_embed = model.predict(en_set)\n",
    "en_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 126, 64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(fr.max_features, embedding_size, input_length=fr.max_length, mask_zero=True))\n",
    "model.compile('rmsprop', 'mse')\n",
    "fr_embed = model.predict(fr_set)\n",
    "fr_embed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Experiment  1\n",
    "hidden_size = 32\n",
    "model = Sequential()\n",
    "model.add(GRU(hidden_size, input_shape=(en_max_length, embedding_size))) # (3000, 110, 64) -> (3000, 32)\n",
    "model.add(RepeatVector(fr_max_length)) # (3000, 32) -> (3000, 126, 32)\n",
    "# model.add(TimeDistributed(Dense(fr_max_length)))\n",
    "model.add(Bidirectional(GRU(embedding_size, return_sequences=True), merge_mode='sum')) # (3000, 126, 32) -> (3000, 126, 64)\n",
    "model.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Experiment  2\n",
    "hidden_size = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(en_max_features, embedding_size, input_length=en_max_length, mask_zero=True))\n",
    "model.add(Bidirectional(GRU(hidden_size), merge_mode='sum'))\n",
    "print(model.input_shape)\n",
    "model.add(RepeatVector(fr_max_length))\n",
    "model.add(GRU(embedding_size))\n",
    "# print(model.output_shape)\n",
    "model.add(Dense(fr_max_length, activation=\"softmax\"))\n",
    "# print(model.output_shape)\n",
    "model.compile('rmsprop', 'mse')\n",
    "model.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(en_set, fr_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Translator(object):\n",
    "    def __init__(self, model, source, target):\n",
    "        self.model = model\n",
    "        self.source = source\n",
    "        self.target = target\n",
    "    \n",
    "    def translate(self, sentence):\n",
    "        ids = sentence_to_token_ids(sentence, self.source)\n",
    "        print(ids)\n",
    "        x = pad_sequences([ids], maxlen=en_max_length)\n",
    "        print(x)\n",
    "        print(model.predict(x))\n",
    "        p = [np.argmax(i) for i in model.predict(x)]\n",
    "        print(p)\n",
    "        return token_ids_to_sentence(p, self.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t = Translator(model, en_vocab, fr_index)\n",
    "t.translate(\"good morning my name is\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model infered from Seq2Seq\n",
    "```python\n",
    "model = Sequential()\n",
    "model.add(LSTM(hidden_dim, return_sequences=True, mask_zero=True))\n",
    "model.add(Dropout(droupout))\n",
    "model.add(LSTM(hidden_dim, )) # Encoder\n",
    "model.add(Dropout(droupout))\n",
    "model.add(RepeatVecotr(output_lenght))\n",
    "model.add(LSTM(hidden_dim, return_sequences=True, )) # Decoder\n",
    "model.add(LSTM(hidden_dim, return_sequences=True, ))\n",
    "model.add(Droupout(droupout))\n",
    "model.add(TimeDistributed(Dense(output_dim)))\n",
    "model.compile('rmsprop', 'mse')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### https://github.com/fchollet/keras/issues/395\n",
    "\n",
    "In the model I listed below with the english sentence as input and the entire french sentence as output. The RNN model will maintain state across each timestep as it predicts the output sentence, no extra work required on your behalf. You will however need to one hot encode and zero pad the output sequence (the french sentence) and have it do a softmax over all possible words for the output at each time step. The ys then are 3D, each row is a matrix of height - number of french words, and width - number of time steps.\n",
    "\n",
    "```python\n",
    "embedding_size = 50\n",
    "hidden_size = 512\n",
    "output_size = 20\n",
    "maxlen = 60\n",
    "\n",
    "model = Sequential()\n",
    "model.add(JZS1(embedding_size, hidden_size)) # try using a GRU instead, for fun\n",
    "model.add(Dense(hidden_size, hidden_size))\n",
    "model.add(Activation('relu'))\n",
    "model.add(RepeatVector(maxlen))\n",
    "model.add(JZS1(hidden_size, hidden_size, return_sequences=True))\n",
    "model.add(TimeDistributedDense(hidden_size, output_size, activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers.wrappers import TimeDistributed\n",
    "\n",
    "embedding_size = 64\n",
    "hidden_size = 512\n",
    "embedding_size = EN_REPRESENTATION_SIZE\n",
    "MAX_LEN = fr_set.shape[1]\n",
    "max_features = FR_BOUND\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(EN_REPRESENTATION_SIZE, EMBED_HIDDEN_SIZE, input_length=en_set.shape[1], mask_zero=True))\n",
    "model.add(GRU(hidden_size)) # try using a GRU instead, for fun\n",
    "model.add(Dense(hidden_size))\n",
    "model.add(Activation('relu'))\n",
    "model.add(RepeatVector(MAX_LEN))\n",
    "model.add(GRU(hidden_size, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(max_features, activation=\"softmax\")))\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy seq2seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seq2seq\n",
    "from seq2seq.models import SimpleSeq2seq\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(EN_REPRESENTATION_SIZE, EMBED_HIDDEN_SIZE, input_length=en_set.shape[1]))\n",
    "model = SimpleSeq2seq(\n",
    "        input_dim=EN_REPRESENTATION_SIZE,\n",
    "        input_length=en_max_length,\n",
    "        hidden_dim=50,\n",
    "        output_length=FR_REPRESENTATION_SIZE,\n",
    "        output_dim=fr_max_length)\n",
    "\n",
    "model.compile(loss='mse', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model.fit(en_embed, fr_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# RNN = GRU\n",
    "# EMBED_HIDDEN_SIZE = 50\n",
    "\n",
    "# encoder = Sequential()\n",
    "# encoder.add(Embedding(en_vocab_size, EMBED_HIDDEN_SIZE, input_length=en_max_length))\n",
    "\n",
    "# decoder = Sequential()\n",
    "# decoder.add(Embedding(fr_vocab_size, EMBED_HIDDEN_SIZE, input_length=fr_max_length))\n",
    "\n",
    "# decoder.add(RNN(EMBED_HIDDEN_SIZE, return_sequences=False))\n",
    "# decoder.add(RepeatVector(en_max_length))\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Merge([encoder, decoder], mode='sum'))\n",
    "# model.add(RNN(EMBED_HIDDEN_SIZE, return_sequences=False))\n",
    "# model.add(Dropout(0.3))\n",
    "# model.add(Dense(fr_vocab_size, activation='softmax'))\n",
    "\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# p = model.predict([en_set, fr_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# outputs = [int(np.argmax(logit, axis=0)) for logit in p[0]][0:fr_input_length]\n",
    "# token_ids_to_sentence(outputs, fr_index)\n",
    "# # [fr_ids[output] for output in outputs]\n",
    "# # print(\" \".join([tf.compat.as_str(fr_ids[output]) for output in outputs]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
