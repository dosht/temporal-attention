{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [WORK IN PROGRESS]\n",
    "# Temporal Attention Model for Neural Machine Translation\n",
    "Unofficial implementation of paper: http://arxiv.org/abs/1608.02927"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading (French, English) language pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.models.rnn.translate import data_utils\n",
    "data_dir = \"/data/translate\" # You may need to change that or create a sympolic link\n",
    "vocab_size = 20000\n",
    "pathes = data_utils.prepare_wmt_data(data_dir, vocab_size, vocab_size)\n",
    "en2_path, fr2_path, en2013_path, fr2013_path, en_vocab_path, fr_vocab_path = pathes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def is_ascii(s):\n",
    "    return all(ord(c) < 128 for c in s)\n",
    "\n",
    "_PAD = \"_PAD\"\n",
    "_GO = \"_GO\"\n",
    "_EOS = \"_EOS\"\n",
    "_UNK = \"_UNK\"\n",
    "_START_VOCAB = [_PAD, _GO, _EOS, _UNK]\n",
    "\n",
    "def read_vocab(vocab_path):\n",
    "    vocab_list = []\n",
    "    vocab_list.extend(_START_VOCAB)\n",
    "\n",
    "    with open(vocab_path, 'br') as f:\n",
    "        vocab_list.extend([s.decode(\"utf-8\").strip() for s in f.readlines() if is_ascii(s)])\n",
    "\n",
    "    words_to_ids = {w:i for (i, w) in enumerate(vocab_list)}\n",
    "    ids_to_words = {i:w for (w, i) in words_to_ids.items()}\n",
    "    return ids_to_words, words_to_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "en_index, en_vocab = read_vocab(en_vocab_path)\n",
    "fr_index, fr_vocab = read_vocab(fr_vocab_path)\n",
    "\n",
    "def token_ids_to_sentence(ids, vocab_inv):\n",
    "    return \" \".join([vocab_inv.get(_id, _UNK) for _id in ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#FIXME: some non-ascii charachters\n",
    "en_vocab_size = len(en_vocab) + 1\n",
    "fr_vocab_size = len(fr_vocab) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19905\n",
      "19905\n"
     ]
    }
   ],
   "source": [
    "print(len(en_vocab))\n",
    "print(len(en_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from data_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15239, 22, 1511, 614, 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'a is me strategy _UNK'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = sentence_to_token_ids(\"A is me strategy stratÃ©gie\", en_vocab)\n",
    "print(ids)\n",
    "token_ids_to_sentence(ids, en_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/data/translate/giga-fren.release2.ids20000.en',\n",
       " '/data/translate/giga-fren.release2.ids20000.fr',\n",
       " '/data/translate/newstest2013.ids20000.en',\n",
       " '/data/translate/newstest2013.ids20000.fr',\n",
       " '/data/translate/vocab20000.en',\n",
       " '/data/translate/vocab20000.fr')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pathes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    with open(path, 'r') as f:\n",
    "        return [[int(x) for x in line.split(\" \")] for line in f.read().splitlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "en_set = read_data(en2013_path)\n",
    "fr_set = read_data(fr2013_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110\n",
      "126\n"
     ]
    }
   ],
   "source": [
    "en_maxlen = max(map(len, en_set))\n",
    "fr_maxlen = max(map(len, fr_set))\n",
    "print(en_maxlen)\n",
    "print(fr_maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Dense, Merge, Dropout, RepeatVector, Permute, Activation, recurrent, LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoder = Sequential()\n",
    "encoder.add(Embedding(en_vocab_size, 50, input_length=en_maxlen))\n",
    "encoder.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.04156312,  0.01821918, -0.04753896, ..., -0.03579432,\n",
       "          0.02079523, -0.04182146],\n",
       "        [ 0.02747966,  0.01740393, -0.01015042, ...,  0.03865925,\n",
       "          0.03215876, -0.00618281],\n",
       "        [-0.02605469, -0.00345312, -0.0356955 , ..., -0.00068561,\n",
       "          0.00706407,  0.00476055],\n",
       "        ..., \n",
       "        [-0.02605469, -0.00345312, -0.0356955 , ..., -0.00068561,\n",
       "          0.00706407,  0.00476055],\n",
       "        [-0.04654215, -0.03888209,  0.03690505, ...,  0.02469091,\n",
       "          0.04567369,  0.02441451],\n",
       "        [ 0.03491415,  0.0054909 ,  0.00489568, ...,  0.00560037,\n",
       "          0.03416372, -0.02973272]]], dtype=float32)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.predict(np.array([x for x in en_set if len(x) == 110]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
