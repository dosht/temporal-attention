{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [WORK IN PROGRESS]\n",
    "# Temporal Attention Model for Neural Machine Translation\n",
    "Unofficial implementation of paper: http://arxiv.org/abs/1608.02927\n",
    "\n",
    "### Requirements:\n",
    " - [Keras](https://github.com/fchollet/keras)\n",
    " - [Tensorflow](https://github.com/tensorflow/tensorflow)\n",
    " - [Theano](https://github.com/Theano/Theano)\n",
    " - https://github.com/farizrahman4u/seq2seq Seq2Seq implemtation built on top of Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading (French, English) language pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.models.rnn.translate import data_utils\n",
    "data_dir = \"/data/translate\" # You may need to change that or create a sympolic link\n",
    "vocab_size = 20000\n",
    "pathes = data_utils.prepare_wmt_data(data_dir, vocab_size, vocab_size)\n",
    "en2_path, fr2_path, en2013_path, fr2013_path, en_vocab_path, fr_vocab_path = pathes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def is_ascii(s):\n",
    "    return all(ord(c) < 128 for c in s)\n",
    "\n",
    "# https://github.com/nicolas-ivanov/tf_seq2seq_chatbot/blob/master/tf_seq2seq_chatbot/lib/data_utils.py\n",
    "\n",
    "_PAD = \"_PAD\"\n",
    "_GO = \"_GO\"\n",
    "_EOS = \"_EOS\"\n",
    "_UNK = \"_UNK\"\n",
    "_START_VOCAB = [_PAD, _GO, _EOS, _UNK]\n",
    "\n",
    "PAD_ID = 0\n",
    "GO_ID = 1\n",
    "EOS_ID = 2\n",
    "UNK_ID = 3\n",
    "\n",
    "# Regular expressions used to tokenize.\n",
    "_WORD_SPLIT = re.compile(\"([.,!?\\\"':;)(])\")\n",
    "_DIGIT_RE = re.compile(r\"\\d{3,}\")\n",
    "\n",
    "def read_vocab(vocab_path):\n",
    "    vocab_list = []\n",
    "    vocab_list.extend(_START_VOCAB)\n",
    "\n",
    "    with open(vocab_path, 'br') as f:\n",
    "        vocab_list.extend([s.decode(\"utf-8\").strip() for s in f.readlines() if is_ascii(s)])\n",
    "\n",
    "    words_to_ids = {w:i for (i, w) in enumerate(vocab_list)}\n",
    "    ids_to_words = {i:w for (w, i) in words_to_ids.items()}\n",
    "    return ids_to_words, words_to_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "en_index, en_vocab = read_vocab(en_vocab_path)\n",
    "fr_index, fr_vocab = read_vocab(fr_vocab_path)\n",
    "\n",
    "def basic_tokenizer(sentence):\n",
    "    \"\"\"Very basic tokenizer: split the sentence into a list of tokens.\"\"\"\n",
    "    words = []\n",
    "    for space_separated_fragment in sentence.strip().split():\n",
    "        words.extend(re.split(_WORD_SPLIT, space_separated_fragment))\n",
    "    return [w.lower() for w in words if w]\n",
    "\n",
    "def sentence_to_token_ids(sentence, vocabulary,\n",
    "                          tokenizer=None, normalize_digits=True):\n",
    "    \"\"\"Convert a string to list of integers representing token-ids.\n",
    "\n",
    "    For example, a sentence \"I have a dog\" may become tokenized into\n",
    "    [\"I\", \"have\", \"a\", \"dog\"] and with vocabulary {\"I\": 1, \"have\": 2,\n",
    "    \"a\": 4, \"dog\": 7\"} this function will return [1, 2, 4, 7].\n",
    "\n",
    "    Args:\n",
    "    sentence: a string, the sentence to convert to token-ids.\n",
    "    vocabulary: a dictionary mapping tokens to integers.\n",
    "    tokenizer: a function to use to tokenize each sentence;\n",
    "      if None, basic_tokenizer will be used.\n",
    "    normalize_digits: Boolean; if true, all digits are replaced by 0s.\n",
    "\n",
    "    Returns:\n",
    "    a list of integers, the token-ids for the sentence.\n",
    "    \"\"\"\n",
    "    if tokenizer:\n",
    "        words = tokenizer(sentence)\n",
    "    else:\n",
    "        words = basic_tokenizer(sentence)\n",
    "    if not normalize_digits:\n",
    "        return [vocabulary.get(w, UNK_ID) for w in words]\n",
    "  \n",
    "    # Normalize digits by 0 before looking words up in the vocabulary.\n",
    "    return [vocabulary.get(re.sub(_DIGIT_RE, \"0\", w), UNK_ID) for w in words]\n",
    "\n",
    "def token_ids_to_sentence(ids, vocab_index):\n",
    "    maybe_words = [vocab_index.get(_id) for _id in ids]\n",
    "    return \" \".join([w for w in maybe_words if w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#FIXME: some non-ascii charachters\n",
    "en_vocab_size = len(en_vocab) + 1\n",
    "fr_vocab_size = len(fr_vocab) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19905\n",
      "19905\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "print(len(en_vocab))\n",
    "print(len(en_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15239, 22, 1511, 614, 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'a is me strategy'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = sentence_to_token_ids(\"A is me strategy stratégie\", en_vocab)\n",
    "print(ids)\n",
    "token_ids_to_sentence(ids, en_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/data/translate/giga-fren.release2.ids20000.en',\n",
       " '/data/translate/giga-fren.release2.ids20000.fr',\n",
       " '/data/translate/newstest2013.ids20000.en',\n",
       " '/data/translate/newstest2013.ids20000.fr',\n",
       " '/data/translate/vocab20000.en',\n",
       " '/data/translate/vocab20000.fr')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pathes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    with open(path, 'r') as f:\n",
    "        return [[int(x) for x in line.split(\" \")] for line in f.read().splitlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# e.g. [59, 3, 610, 9, 6251, 4, 3, 7, 3]\n",
    "en_ids = read_data(en2013_path)\n",
    "fr_ids = read_data(fr2013_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX 980M (CNMeM is disabled, cuDNN 5005)\n"
     ]
    }
   ],
   "source": [
    "# Make it the same length (= the max length of the sentences) with zeros for shorter sentences\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "en_set = pad_sequences(en_ids)\n",
    "fr_set = pad_sequences(fr_ids)\n",
    "en_max_length = en_set.shape[1]\n",
    "fr_max_length = fr_set.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'base mieux génétiquement du _UNK'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids_to_sentence(fr_set[0], fr_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Dense, Merge, Dropout, RepeatVector, Permute, Activation, recurrent, LSTM, GRU\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeding layer for en and fr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 110, 50)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EN_BOUND = max(en_vocab.values())\n",
    "model = Sequential()\n",
    "model.add(Embedding(EN_BOUND, EMBED_HIDDEN_SIZE, input_length=en_set.shape[1]))\n",
    "model.compile('rmsprop', 'mse')\n",
    "en_embed = model.predict(en_set)\n",
    "en_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 126, 50)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FR_BOUND = max(fr_vocab.values())\n",
    "model = Sequential()\n",
    "model.add(Embedding(FR_BOUND, EMBED_HIDDEN_SIZE, input_length=fr_set.shape[1]))\n",
    "model.compile('rmsprop', 'mse')\n",
    "fr_embed = model.predict(fr_set)\n",
    "fr_embed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy seq2seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seq2seq\n",
    "from seq2seq.models import SimpleSeq2seq\n",
    "\n",
    "model = SimpleSeq2seq(input_dim=en_max_length, hidden_dim=50, output_length=fr_vocab_size, output_dim=fr_max_length)\n",
    "model.compile(loss='mse', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Error when checking model input: expected lstm_input_4 to have shape (None, None, 110) but got array with shape (3000, 110, 50)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-34d037db9e7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men_embed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfr_embed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, **kwargs)\u001b[0m\n\u001b[1;32m    594\u001b[0m                               \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m                               sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight)\u001b[0m\n\u001b[1;32m   1033\u001b[0m                                                            \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m                                                            \u001b[0mcheck_batch_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1035\u001b[0;31m                                                            batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1036\u001b[0m         \u001b[0;31m# prepare validation data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_dim, batch_size)\u001b[0m\n\u001b[1;32m    960\u001b[0m                                    \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m                                    \u001b[0mcheck_batch_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 962\u001b[0;31m                                    exception_prefix='model input')\n\u001b[0m\u001b[1;32m    963\u001b[0m         y = standardize_input_data(y, self.output_names,\n\u001b[1;32m    964\u001b[0m                                    \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_dim, exception_prefix)\u001b[0m\n\u001b[1;32m    106\u001b[0m                                         \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                                         \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m                                         str(array.shape))\n\u001b[0m\u001b[1;32m    109\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Error when checking model input: expected lstm_input_4 to have shape (None, None, 110) but got array with shape (3000, 110, 50)"
     ]
    }
   ],
   "source": [
    "model.fit(en_embed, fr_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# RNN = GRU\n",
    "# EMBED_HIDDEN_SIZE = 50\n",
    "\n",
    "# encoder = Sequential()\n",
    "# encoder.add(Embedding(en_vocab_size, EMBED_HIDDEN_SIZE, input_length=en_max_length))\n",
    "\n",
    "# decoder = Sequential()\n",
    "# decoder.add(Embedding(fr_vocab_size, EMBED_HIDDEN_SIZE, input_length=fr_max_length))\n",
    "\n",
    "# decoder.add(RNN(EMBED_HIDDEN_SIZE, return_sequences=False))\n",
    "# decoder.add(RepeatVector(en_max_length))\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Merge([encoder, decoder], mode='sum'))\n",
    "# model.add(RNN(EMBED_HIDDEN_SIZE, return_sequences=False))\n",
    "# model.add(Dropout(0.3))\n",
    "# model.add(Dense(fr_vocab_size, activation='softmax'))\n",
    "\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# p = model.predict([en_set, fr_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# outputs = [int(np.argmax(logit, axis=0)) for logit in p[0]][0:fr_input_length]\n",
    "# token_ids_to_sentence(outputs, fr_index)\n",
    "# # [fr_ids[output] for output in outputs]\n",
    "# # print(\" \".join([tf.compat.as_str(fr_ids[output]) for output in outputs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
